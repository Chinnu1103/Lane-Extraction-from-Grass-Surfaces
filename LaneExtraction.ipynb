{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LaneExtraction.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSwVkI-mH1Ab",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "import cv2\n",
        "import math\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_87ETvGH31w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "from tensorflow.keras.layers import Conv2D, ZeroPadding2D, Input, BatchNormalization, Activation, Add\n",
        "from tensorflow.keras.layers import AveragePooling2D, Flatten, Conv2DTranspose, Concatenate, MaxPool2D, SeparableConv2D"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aM-yFfEmycdg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _parse_image_function(example_proto):\n",
        "    \"\"\"Mapping function for parsing images and annotations from the tfrecord files. Transforms and augments images and annotations before training.\n",
        "        Args:\n",
        "            example_proto: a single element from the dataset\n",
        "        Returns:\n",
        "            image: transformed image extracted from the datatset\n",
        "            annotation: transformed annotation extracted from the datatset\n",
        "    \"\"\"\n",
        "    \n",
        "    #This descibes the structure of each element in the dataset\n",
        "    image_feature_description={\n",
        "        'height': tf.FixedLenFeature([], tf.int64),\n",
        "        'width': tf.FixedLenFeature([], tf.int64),\n",
        "        'image_raw': tf.FixedLenFeature([], tf.string),\n",
        "        'mask_raw': tf.FixedLenFeature([], tf.string)\n",
        "        }\n",
        "    \n",
        "    #The features are extracted to a dictionary\n",
        "    feature=tf.parse_single_example(example_proto, image_feature_description)\n",
        "    \n",
        "    #Images and Annotations are resized to (192,192)\n",
        "    image = tf.image.decode_jpeg(feature['image_raw'])\n",
        "    image = tf.cast(image, tf.float32) / 255.0\n",
        "    annotation = tf.image.decode_png(feature['mask_raw'], channels=1)\n",
        "    annotation = tf.cast(annotation, tf.float32) / 1.0\n",
        "    \n",
        "    image = tf.reshape(image, (360,640,3))\n",
        "    annotation = tf.reshape(annotation, (360,640,1))\n",
        "    \n",
        "    image = tf.image.pad_to_bounding_box(image, 140, 0, 640, 640)\n",
        "    annotation = tf.image.pad_to_bounding_box(annotation, 140, 0, 640, 640)\n",
        "    \n",
        "    image = tf.image.resize(image, size=(192,192))\n",
        "    annotation = tf.image.resize(annotation, size=(192,192))\n",
        "    \n",
        "    #Randomly flips images and annotations\n",
        "    if(random.random() > 0.5):\n",
        "        image = tf.image.flip_left_right(image)\n",
        "        annotation = tf.image.flip_left_right(annotation)\n",
        "    \n",
        "    return image, annotation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqThk5wL4xqP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Defines the batch size of the dataset\n",
        "BATCH_SIZE = 8"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xc77Ezi33hO5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Decodes and maps the tfrecord files into a Dataset object for training, validation and evaluation\n",
        "\n",
        "train_dataset = tf.data.TFRecordDataset(['train.tfrecords'])\n",
        "train_dataset = train_dataset.map(_parse_image_function)\n",
        "ds_train = train_dataset.shuffle(buffer_size=584)\n",
        "ds_train = ds_train.repeat()\n",
        "ds_train = ds_train.batch(BATCH_SIZE)\n",
        "ds_train = ds_train.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "val_dataset = tf.data.TFRecordDataset('val.tfrecords')\n",
        "val_dataset = val_dataset.map(_parse_image_function)\n",
        "ds_val = val_dataset.shuffle(buffer_size=32)\n",
        "ds_val = ds_val.repeat()\n",
        "ds_val = ds_val.batch(BATCH_SIZE)\n",
        "ds_val = ds_val.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "test_dataset = tf.data.TFRecordDataset('test.tfrecords')\n",
        "test_dataset = test_dataset.map(_parse_image_function)\n",
        "ds_test = test_dataset.shuffle(buffer_size=34)\n",
        "ds_test = ds_test.repeat()\n",
        "ds_test = ds_test.batch(BATCH_SIZE)\n",
        "ds_test = ds_test.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-gL8zQQ3pLx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def segmentationModel():\n",
        "    \"\"\"Function to define the entire encoder-decoder structure for the model\n",
        "        Args:\n",
        "            None\n",
        "        Returns:\n",
        "            model: the keras model for semantic segmentation\n",
        "    \"\"\"\n",
        "    \n",
        "    #input layer\n",
        "    input = Input(shape=(192,192,3), name='input')\n",
        "    zero_pad = ZeroPadding2D((4,4))(input)\n",
        "    \n",
        "    ############   ENCODER   #######################\n",
        "    \n",
        "    #normal convolution layer\n",
        "    conv_0 = Conv2D(64, (3,3), strides=(1,1), padding='same', name='conv_0')(zero_pad)\n",
        "    bn_0 = BatchNormalization(axis=3, name='bn_0')(conv_0)\n",
        "    actv_0 = Activation('relu')(bn_0)\n",
        "    pool_0 = SeparableConv2D(64, (2,2), strides=(2,2), padding='valid')(actv_0)\n",
        "    \n",
        "    #first convolution block\n",
        "    conv_1_a = Conv2D(64, (3,3), strides=(1,1), padding='valid', name='conv_1_a')(pool_0)\n",
        "    bn_1_a = BatchNormalization(axis=3, name='bn_1_a')(conv_1_a)\n",
        "    actv_1_a = Activation('elu')(bn_1_a)\n",
        "    conv_1_b = Conv2D(64, (3,3), strides=(1,1), padding='same', dilation_rate=(3,3), name='conv_1_b')(actv_1_a)\n",
        "    bn_1_b = BatchNormalization(axis=3, name='bn_1_b')(conv_1_b)\n",
        "    actv_1_b = Activation('elu')(bn_1_b)\n",
        "    conv_1_c = Conv2D(256, (1,1), strides=(1,1), padding='valid', name='conv_1_c')(actv_1_b)\n",
        "    bn_1_c = BatchNormalization(axis=3, name='bn_1_c')(conv_1_c)\n",
        "    conv_1_s = Conv2D(256, (3,3), strides=(1,1), padding='valid', name='conv_1_s')(pool_0)\n",
        "    bn_1_s = BatchNormalization(axis=3, name='bn_1_s')(conv_1_s)\n",
        "    actv_1_c = Add()([bn_1_c, bn_1_s])\n",
        "    actv_1_c = Activation('relu')(actv_1_c)\n",
        "    #Two Identity blocks\n",
        "    conv_2_a = Conv2D(64, (1,1), strides=(1,1), padding='valid', name='conv_2_a')(actv_1_c)\n",
        "    bn_2_a = BatchNormalization(axis=3, name='bn_2_a')(conv_2_a)\n",
        "    actv_2_a = Activation('elu')(bn_2_a)\n",
        "    conv_2_b = Conv2D(64, (3,3), strides=(1,1), padding='same', name='conv_2_b')(actv_2_a)\n",
        "    bn_2_b = BatchNormalization(axis=3, name='bn_2_b')(conv_2_b)\n",
        "    actv_2_b = Activation('elu')(bn_2_b)\n",
        "    conv_2_c = Conv2D(256, (1,1), strides=(1,1), padding='valid', name='conv_2_c')(actv_2_b)\n",
        "    bn_2_c = BatchNormalization(axis=3, name='bn_2_c')(conv_2_c)\n",
        "    actv_2_c = Add()([bn_2_c, actv_1_c])\n",
        "    actv_2_c = Activation('relu')(actv_2_c)\n",
        "    #-----------------------------------------------------------------------------------\n",
        "    conv_3_a = Conv2D(64, (1,1), strides=(1,1), padding='valid', name='conv_3_a')(actv_2_c)\n",
        "    bn_3_a = BatchNormalization(axis=3, name='bn_3_a')(conv_3_a)\n",
        "    actv_3_a = Activation('elu')(bn_3_a)\n",
        "    conv_3_b = Conv2D(64, (3,3), strides=(1,1), padding='same', name='conv_3_b')(actv_3_a)\n",
        "    bn_3_b = BatchNormalization(axis=3, name='bn_3_b')(conv_3_b)\n",
        "    actv_3_b = Activation('elu')(bn_3_b)\n",
        "    conv_3_c = Conv2D(256, (1,1), strides=(1,1), padding='valid', name='conv_3_c')(actv_3_b)\n",
        "    bn_3_c = BatchNormalization(axis=3, name='bn_3_c')(conv_3_c)\n",
        "    actv_3_c = Add()([bn_3_c, actv_2_c])\n",
        "    actv_3_c = Activation('relu')(actv_3_c)\n",
        "    pool_1 = SeparableConv2D(256, (2,2), strides=(2,2), padding='valid')(actv_3_c)\n",
        "    #-----------------------------------------------------------------------------------\n",
        "    \n",
        "    #Second convolution block\n",
        "    conv_4_a = Conv2D(128, (3,3), strides=(1,1), padding='valid', name='conv_4_a')(pool_1)\n",
        "    bn_4_a = BatchNormalization(axis=3, name='bn_4_a')(conv_4_a)\n",
        "    actv_4_a = Activation('elu')(bn_4_a)\n",
        "    conv_4_b = Conv2D(128, (3,3), strides=(1,1), padding='same', dilation_rate=(3,3), name='conv_4_b')(actv_4_a)\n",
        "    bn_4_b = BatchNormalization(axis=3, name='bn_4_b')(conv_4_b)\n",
        "    actv_4_b = Activation('elu')(bn_4_b)\n",
        "    conv_4_c = Conv2D(512, (1,1), strides=(1,1), padding='valid', name='conv_4_c')(actv_4_b)\n",
        "    bn_4_c = BatchNormalization(axis=3, name='bn_4_c')(conv_4_c)\n",
        "    conv_4_s = Conv2D(512, (3,3), strides=(1,1), padding='valid', name='conv_4_s')(pool_1)\n",
        "    bn_4_s = BatchNormalization(axis=3, name='bn_4_s')(conv_4_s)\n",
        "    actv_4_c = Add()([bn_4_c, bn_4_s])\n",
        "    actv_4_c = Activation('relu')(actv_4_c)\n",
        "    #Three Identity blocks\n",
        "    conv_5_a = Conv2D(128, (1,1), strides=(1,1), padding='valid', name='conv_5_a')(actv_4_c)\n",
        "    bn_5_a = BatchNormalization(axis=3, name='bn_5_a')(conv_5_a)\n",
        "    actv_5_a = Activation('elu')(bn_5_a)\n",
        "    conv_5_b = Conv2D(128, (3,3), strides=(1,1), padding='same', name='conv_5_b')(actv_5_a)\n",
        "    bn_5_b = BatchNormalization(axis=3, name='bn_5_b')(conv_5_b)\n",
        "    actv_5_b = Activation('elu')(bn_5_b)\n",
        "    conv_5_c = Conv2D(512, (1,1), strides=(1,1), padding='valid', name='conv_5_c')(actv_5_b)\n",
        "    bn_5_c = BatchNormalization(axis=3, name='bn_5_c')(conv_5_c)\n",
        "    actv_5_c = Add()([bn_5_c, actv_4_c])\n",
        "    actv_5_c = Activation('relu')(actv_5_c)\n",
        "    #-----------------------------------------------------------------------------------\n",
        "    conv_6_a = Conv2D(128, (1,1), strides=(1,1), padding='valid', name='conv_6_a')(actv_5_c)\n",
        "    bn_6_a = BatchNormalization(axis=3, name='bn_6_a')(conv_6_a)\n",
        "    actv_6_a = Activation('elu')(bn_6_a)\n",
        "    conv_6_b = Conv2D(128, (3,3), strides=(1,1), padding='same', name='conv_6_b')(actv_6_a)\n",
        "    bn_6_b = BatchNormalization(axis=3, name='bn_6_b')(conv_6_b)\n",
        "    actv_6_b = Activation('elu')(bn_6_b)\n",
        "    conv_6_c = Conv2D(512, (1,1), strides=(1,1), padding='valid', name='conv_6_c')(actv_6_b)\n",
        "    bn_6_c = BatchNormalization(axis=3, name='bn_6_c')(conv_6_c)\n",
        "    actv_6_c = Add()([bn_6_c, actv_5_c])\n",
        "    actv_6_c = Activation('relu')(actv_6_c)\n",
        "    #-----------------------------------------------------------------------------------\n",
        "    conv_7_a = Conv2D(128, (1,1), strides=(1,1), padding='valid', name='conv_7_a')(actv_6_c)\n",
        "    bn_7_a = BatchNormalization(axis=3, name='bn_7_a')(conv_7_a)\n",
        "    actv_7_a = Activation('elu')(bn_7_a)\n",
        "    conv_7_b = Conv2D(128, (3,3), strides=(1,1), padding='same', name='conv_7_b')(actv_7_a)\n",
        "    bn_7_b = BatchNormalization(axis=3, name='bn_7_b')(conv_7_b)\n",
        "    actv_7_b = Activation('elu')(bn_7_b)\n",
        "    conv_7_c = Conv2D(512, (1,1), strides=(1,1), padding='valid', name='conv_7_c')(actv_7_b)\n",
        "    bn_7_c = BatchNormalization(axis=3, name='bn_7_c')(conv_7_c)\n",
        "    actv_7_c = Add()([bn_7_c, actv_6_c])\n",
        "    actv_7_c = Activation('relu')(actv_7_c)\n",
        "    pool_2 = SeparableConv2D(512, (2,2), strides=(2,2), padding='valid')(actv_7_c)\n",
        "    #-----------------------------------------------------------------------------------\n",
        "    \n",
        "    #third convolution block\n",
        "    conv_8_a = Conv2D(256, (3,3), strides=(1,1), padding='same', name='conv_8_a')(pool_2)\n",
        "    bn_8_a = BatchNormalization(axis=3, name='bn_8_a')(conv_8_a)\n",
        "    actv_8_a = Activation('elu')(bn_8_a)\n",
        "    conv_8_b = Conv2D(256, (3,3), strides=(1,1), padding='same', dilation_rate=(3,3), name='conv_8_b')(actv_8_a)\n",
        "    bn_8_b = BatchNormalization(axis=3, name='bn_8_b')(conv_8_b)\n",
        "    actv_8_b = Activation('elu')(bn_8_b)\n",
        "    conv_8_c = Conv2D(1024, (1,1), strides=(1,1), padding='valid', name='conv_8_c')(actv_8_b)\n",
        "    bn_8_c = BatchNormalization(axis=3, name='bn_8_c')(conv_8_c)\n",
        "    conv_8_s = Conv2D(1024, (3,3), strides=(1,1), padding='same', name='conv_8_s')(pool_2)\n",
        "    bn_8_s = BatchNormalization(axis=3, name='bn_8_s')(conv_8_s)\n",
        "    actv_8_c = Add()([bn_8_c, bn_8_s])\n",
        "    actv_8_c = Activation('relu')(actv_8_c)\n",
        "    #Two Identity blocks\n",
        "    conv_9_a = Conv2D(256, (1,1), strides=(1,1), padding='valid', name='conv_9_a')(actv_8_c)\n",
        "    bn_9_a = BatchNormalization(axis=3, name='bn_9_a')(conv_9_a)\n",
        "    actv_9_a = Activation('elu')(bn_9_a)\n",
        "    conv_9_b = Conv2D(256, (3,3), strides=(1,1), padding='same', name='conv_9_b')(actv_9_a)\n",
        "    bn_9_b = BatchNormalization(axis=3, name='bn_9_b')(conv_9_b)\n",
        "    actv_9_b = Activation('elu')(bn_9_b)\n",
        "    conv_9_c = Conv2D(1024, (1,1), strides=(1,1), padding='valid', name='conv_9_c')(actv_9_b)\n",
        "    bn_9_c = BatchNormalization(axis=3, name='bn_9_c')(conv_9_c)\n",
        "    actv_9_c = Add()([bn_9_c, actv_8_c])\n",
        "    actv_9_c = Activation('relu')(actv_9_c)\n",
        "    #-----------------------------------------------------------------------------------\n",
        "    conv_10_a = Conv2D(256, (1,1), strides=(1,1), padding='valid', name='conv_10_a')(actv_9_c)\n",
        "    bn_10_a = BatchNormalization(axis=3, name='bn_10_a')(conv_10_a)\n",
        "    actv_10_a = Activation('elu')(bn_10_a)\n",
        "    conv_10_b = Conv2D(256, (3,3), strides=(1,1), padding='same', name='conv_10_b')(actv_10_a)\n",
        "    bn_10_b = BatchNormalization(axis=3, name='bn_10_b')(conv_10_b)\n",
        "    actv_10_b = Activation('elu')(bn_10_b)\n",
        "    conv_10_c = Conv2D(1024, (1,1), strides=(1,1), padding='valid', name='conv_10_c')(actv_10_b)\n",
        "    bn_10_c = BatchNormalization(axis=3, name='bn_10_c')(conv_10_c)\n",
        "    actv_10_c = Add()([bn_10_c, actv_9_c])\n",
        "    actv_10_c = Activation('relu')(actv_10_c)\n",
        "    actv_10_c = ZeroPadding2D(((0, 1), (0, 1)))(actv_10_c)\n",
        "    #-----------------------------------------------------------------------------------\n",
        "    \n",
        "    #Atrous Convolutions\n",
        "    conv_final_a = Conv2D(256, (1,1), strides=(1,1), padding='same', name='conv_final_a')(actv_10_c)\n",
        "    conv_final_a = AveragePooling2D((3,3), strides=(1,1), padding='same')(conv_final_a)\n",
        "    bn_final_a = BatchNormalization(axis=3, name='bn_final_a')(conv_final_a)\n",
        "    conv_final_b = Conv2D(256, (3,3), strides=(1,1), padding='same', dilation_rate=(3,3), name='conv_final_b')(actv_10_c)\n",
        "    bn_final_b = BatchNormalization(axis=3, name='bn_final_b')(conv_final_b)\n",
        "    conv_final_c = Conv2D(256, (3,3), strides=(1,1), padding='same', dilation_rate=(5,5), name='conv_final_c')(actv_10_c)\n",
        "    bn_final_c = BatchNormalization(axis=3, name='bn_final_c')(conv_final_c)\n",
        "    conv_final_d = Conv2D(256, (3,3), strides=(1,1), padding='same', dilation_rate=(7,7), name='conv_final_d')(actv_10_c)\n",
        "    bn_final_d = BatchNormalization(axis=3, name='bn_final_d')(conv_final_d)\n",
        "    \n",
        "    actv_final = Concatenate()([bn_final_a, bn_final_b, bn_final_c, bn_final_d])\n",
        "    actv_final = Conv2D(1024, (3,3), strides=(1,1), padding='same', activation='elu')(actv_final)\n",
        "    actv_final = Conv2D(512, (1,1), strides=(1,1), padding='valid')(actv_final)\n",
        "    actv_final = Activation('relu')(actv_final)\n",
        "    \n",
        "    ###############   DECODER   ####################\n",
        "    \n",
        "    conv_up_a = Conv2DTranspose(512, kernel_size=(3,3), strides=(2, 2), padding='same', activation='relu')(actv_final)\n",
        "    actv_7_c = ZeroPadding2D(((0, 1), (0, 1)))(actv_7_c)\n",
        "    conv_join_a = Concatenate()([actv_7_c, conv_up_a])\n",
        "    conv_join_a = Conv2D(512, (3,3), strides=(1,1), padding='same', activation='elu')(conv_join_a)\n",
        "    conv_join_a = Conv2D(256, (1,1), strides=(1,1), padding='same', activation='elu')(conv_join_a)\n",
        "    conv_join_a = BatchNormalization(axis=3)(conv_join_a)\n",
        "    \n",
        "    conv_up_b = Conv2DTranspose(256, kernel_size=(3,3), strides=(2, 2), padding='same', activation='relu')(conv_join_a)\n",
        "    conv_join_b = Conv2D(256, (3,3), strides=(1,1), padding='same', activation='elu')(conv_up_b)\n",
        "    conv_join_b = Conv2D(128, (1,1), strides=(1,1), padding='same', activation='elu')(conv_join_b)\n",
        "    conv_join_b = BatchNormalization(axis=3)(conv_join_b)\n",
        "    \n",
        "    output = Conv2DTranspose(128, kernel_size=(3,3), strides=(2, 2), padding='same', activation='relu')(conv_join_b)    \n",
        "    output = Conv2D(128, (3,3), strides=(1,1), padding='same', activation='relu')(output)\n",
        "    output = Conv2D(1, (1,1), strides=(1,1), padding='same', activation='sigmoid')(output)\n",
        "    \n",
        "    model = Model(inputs=input, outputs=output, name='laneModel')    \n",
        "    return model\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSJeAZEU5Fl7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = segmentationModel()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzjdON-Cl-th",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def step_decay(epoch):\n",
        "    \"\"\"Fuction to define the decaying of learning rate in a step-wise manner\n",
        "        Args:\n",
        "            epoch: the current epoch that the model is training on\n",
        "        Returns:\n",
        "            lrate: the corresponding learning rate for the epoch\n",
        "    \"\"\"\n",
        "    \n",
        "\tinitial_lrate = 0.001\n",
        "\tdrop = 0.8\n",
        "\tepochs_drop = 2\n",
        "\tlrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
        "\treturn lrate\n",
        "\n",
        "#learning rate with decay dfined by above funtion\n",
        "lrate = LearningRateScheduler(step_decay)\n",
        "\n",
        "#Callback to stop training after accuracy reaches 99.5%\n",
        "class myCallBack(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        if(logs.get('acc') >= 0.995):\n",
        "            self.model.stop_training = True\n",
        "\n",
        "callback = myCallBack()\n",
        "\n",
        "#Final list of callbacks    \n",
        "callbacks_list = [lrate, callback]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TuSV6IcfDN0x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Compile the model with Adam optimizer and crossentropy loss\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(0.0), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "#Run the model on the train dataset and check it's performance on the validation datatset\n",
        "history = model.fit(ds_train.make_one_shot_iterator(),steps_per_epoch=600//BATCH_SIZE,epochs=50, validation_data=ds_val.make_one_shot_iterator(), validation_steps=1, callbacks=callbacks_list, verbose = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ghv3EWktp5X-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Visualization for model accuracy and loss\n",
        "\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8bmJFF-8ZdV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Testing the model on the test datatset\n",
        "model.evaluate(ds_test.make_one_shot_iterator(),batch_size=16, steps=20,verbose = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YKG5OiwMeFKC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Save the h5 file of the model\n",
        "model.save('lanes.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}